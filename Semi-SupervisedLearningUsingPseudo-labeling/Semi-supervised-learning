library(tidyverse)
library(caret)
library(xgboost)
library(RSSL)
library(data.table)

# Load the data
train <- fread("train.csv")
test <- fread("test.csv")


# Remove ID column
train <- train %>% select(-ID)
test_ids <- test$ID
test <- test %>% select(-ID)

# Handling missing values (replace with median)
train[is.na(train)] <- median(train$y, na.rm = TRUE)


train <- train %>% mutate_if(is.factor, as.factor())
test <- test %>% mutate_if(is.factor, as.factor)
# Convert categorical variables to numeric

# mutate_if(): This function from the dplyr package applies a transformation to all columns that meet a certain condition.
# is.character: This checks whether a column is of type character (i.e., a text column).
# as.numeric: Converts character columns into numeric values
# Machine learning models like XGBoost and many others work better with numeric representations of categorical variables.
train <- train %>% mutate_if(is.factor, as.numeric)
test <- test %>% mutate_if(is.factor, as.numeric)


# Splitting labeled and unlabeled data
# Only 20% of the dataset is selected as labeled data
set.seed(123)
labeled_idx <- createDataPartition(train$y, p = 0.2, list = FALSE)
labeled_data <- train[labeled_idx, ]
unlabeled_data <- train[-labeled_idx, ] # Selects the remaining 80% as unlabeled data (data where y will be predicted using pseudo-labeling).


# Train an initial XGBoost model on labeled data
train_matrix <- xgb.DMatrix(data = as.matrix(select(labeled_data, -y)), label = labeled_data$y) # his function converts data into an optimized format for XGBoost, which speeds up training.
params <- list(objective = "reg:squarederror", eval_metric = "rmse") # This means the model is performing regression (predicting a continuous value)
model <- xgb.train(params, train_matrix, nrounds = 100) #  Trains the model using the specified parameters, 
# nrounds = 100: Specifies that the model should run for 100 boosting iterations.

# Predict on unlabeled data
unlabeled_matrix <- xgb.DMatrix(data = as.matrix(select(unlabeled_data, -y)))
pseudo_labels <- predict(model, unlabeled_matrix) # Uses the previously trained XGBoost model to predict target values (y) for the unlabeled data.

# Select confident pseudo-labels: filters the most confident predictions from the pseudo-labeled dataset. These highly confident predictions will be added back to the labeled dataset for training in the next iteration
threshold <- 0.9 # This sets a confidence threshold at 90%.
# It means we will select the top 10% highest and 10% lowest predicted values as the most confident pseudo-labels.
confident_idx <- which(pseudo_labels > quantile(pseudo_labels, threshold) |
                         pseudo_labels < quantile(pseudo_labels, 1 - threshold))
new_labeled <- unlabeled_data[confident_idx, ] # Selects the subset of unlabeled_data that corresponds to confident predictions.
new_labeled$y <- pseudo_labels[confident_idx] # Assigns the predicted values (pseudo_labels[confident_idx]) as new labels for the selected confident data points.

# Merge pseudo-labeled data with labeled data
labeled_data <- rbind(labeled_data, new_labeled)

# Retrain XGBoost with expanded dataset
train_matrix <- xgb.DMatrix(data = as.matrix(select(labeled_data, -y)), label = labeled_data$y)
model <- xgb.train(params, train_matrix, nrounds = 100)

# Predict on test data
test_matrix <- xgb.DMatrix(data = as.matrix(test))
test_predictions <- predict(model, test_matrix)

# Create submission file
submission <- data.frame(ID = test_ids, y = test_predictions)
write.csv(submission, "submission.csv", row.names = FALSE)